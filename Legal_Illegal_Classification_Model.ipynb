{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenyuxiang\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time  \n",
    "from sklearn import metrics  \n",
    "import pickle as pickle  \n",
    "from sklearn.cross_validation import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_file_path):  \n",
    "    \n",
    "    training_df = pd.read_csv(data_file_path, header=0)\n",
    "    feature_cols = ['T0101','T0102','T0201','T0202','T0203','T0204','T0205','T0206','T0207','T0208','T0209','T0210','T0211','T0212','T0213',\n",
    "'T0301','T0302','T0303','T0304','T0305','T0306','T0307','T0308','T0309']\n",
    "    x = training_df[feature_cols]\n",
    "    y = training_df['LEGAL_RECORD']\n",
    "    x_train,x_test, y_train, y_test = train_test_split(x, y, random_state=1) \n",
    "    \n",
    "    return training_df, x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/chenyuxiang/Desktop/Legal_Illegal_Record_all_data.csv\"  \n",
    "\n",
    "all_data_df, train_x, train_y, test_x, test_y = read_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "******************* NB ********************\n",
      "training took 3.811000s!\n",
      "accuracy: 97.58%\n",
      "******************* LR ********************\n",
      "training took 8.074000s!\n",
      "accuracy: 97.59%\n",
      "******************* RF ********************\n",
      "training took 8.715000s!\n",
      "accuracy: 97.65%\n",
      "******************* DT ********************\n",
      "training took 3.997000s!\n",
      "accuracy: 97.64%\n",
      "******************* SVM ********************\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "from sklearn import metrics  \n",
    "import pickle as pickle  \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split  #引用交叉验证  \n",
    "\n",
    "# 定义8种不同的分类学习模型、\n",
    "\n",
    "# Multinomial Naive Bayes Classifier  \n",
    "def naive_bayes_classifier(train_x, train_y):  \n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# KNN Classifier  \n",
    "def knn_classifier(train_x, train_y):  \n",
    "    from sklearn.neighbors import KNeighborsClassifier  \n",
    "    model = KNeighborsClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Logistic Regression Classifier  \n",
    "def logistic_regression_classifier(train_x, train_y):  \n",
    "    from sklearn.linear_model import LogisticRegression  \n",
    "    model = LogisticRegression(penalty='l2')  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Random Forest Classifier  \n",
    "def random_forest_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import RandomForestClassifier  \n",
    "    model = RandomForestClassifier(n_estimators=8)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Decision Tree Classifier  \n",
    "def decision_tree_classifier(train_x, train_y):  \n",
    "    from sklearn import tree  \n",
    "    model = tree.DecisionTreeClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier  \n",
    "def gradient_boosting_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import GradientBoostingClassifier  \n",
    "    model = GradientBoostingClassifier(n_estimators=200)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# SVM Classifier  \n",
    "def svm_classifier(train_x, train_y):  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "# SVM Classifier using cross validation  \n",
    "def svm_cross_validation(train_x, train_y):  \n",
    "    from sklearn.grid_search import GridSearchCV  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}  \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)  \n",
    "    grid_search.fit(train_x, train_y)  \n",
    "    best_parameters = grid_search.best_estimator_.get_params()  \n",
    "    for para, val in list(best_parameters.items()):  \n",
    "        print(para, val)  \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    " \n",
    "thresh = 0.5  \n",
    "\n",
    "model_save = {}  \n",
    "test_classifiers = ['NB', 'LR', 'RF', 'DT', 'SVM', 'GBDT']  \n",
    "#test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'SVM','SVMCV', 'GBDT']  \n",
    "classifiers = {'NB':naive_bayes_classifier,   \n",
    "                  'KNN':knn_classifier,  \n",
    "                   'LR':logistic_regression_classifier,  \n",
    "                   'RF':random_forest_classifier,  \n",
    "                   'DT':decision_tree_classifier,  \n",
    "                  'SVM':svm_classifier,  \n",
    "                'SVMCV':svm_cross_validation,  \n",
    "                 'GBDT':gradient_boosting_classifier  \n",
    "}  \n",
    "      \n",
    "print('reading training and testing data...')  \n",
    "all_data, train_x, train_y, test_x, test_y = read_data(data_file)  \n",
    "      \n",
    "for classifier in test_classifiers:  \n",
    "    print('******************* %s ********************' % classifier)  \n",
    "    start_time = time.time()  \n",
    "    model = classifiers[classifier](train_x, train_y)  \n",
    "    print('training took %fs!' % (time.time() - start_time))  \n",
    "    predict = model.predict(test_x)  \n",
    "    model_save[classifier] = model  \n",
    "    #precision = metrics.precision_score(test_y, predict)  \n",
    "    #recall = metrics.recall_score(test_y, predict)  \n",
    "    #print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))  \n",
    "    accuracy = metrics.accuracy_score(test_y, predict)  \n",
    "    print('accuracy: %.2f%%' % (100 * accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading training and testing data...\n",
    "# ******************* NB ********************\n",
    "# training took 3.848385s!\n",
    "# accuracy: 97.58%\n",
    "# ******************* LR ********************\n",
    "# training took 8.585859s!\n",
    "# accuracy: 97.59%\n",
    "# ******************* RF ********************\n",
    "# training took 8.818882s!\n",
    "# accuracy: 97.64%\n",
    "# ******************* DT ********************\n",
    "# training took 3.926393s!\n",
    "# accuracy: 97.64%\n",
    "# ******************* SVM ********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MultinomialNB in module sklearn.naive_bayes object:\n",
      "\n",
      "class MultinomialNB(BaseDiscreteNB)\n",
      " |  Naive Bayes classifier for multinomial models\n",
      " |  \n",
      " |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      " |  discrete features (e.g., word counts for text classification). The\n",
      " |  multinomial distribution normally requires integer feature counts. However,\n",
      " |  in practice, fractional counts such as tf-idf may also work.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, optional (default=1.0)\n",
      " |      Additive (Laplace/Lidstone) smoothing parameter\n",
      " |      (0 for no smoothing).\n",
      " |  \n",
      " |  fit_prior : boolean, optional (default=True)\n",
      " |      Whether to learn class prior probabilities or not.\n",
      " |      If false, a uniform prior will be used.\n",
      " |  \n",
      " |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_log_prior_ : array, shape (n_classes, )\n",
      " |      Smoothed empirical log probability for each class.\n",
      " |  \n",
      " |  intercept_ : property\n",
      " |      Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
      " |      as a linear model.\n",
      " |  \n",
      " |  feature_log_prob_ : array, shape (n_classes, n_features)\n",
      " |      Empirical log probability of features\n",
      " |      given a class, ``P(x_i|y)``.\n",
      " |  \n",
      " |  coef_ : property\n",
      " |      Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
      " |      as a linear model.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      Number of samples encountered for each class during fitting. This\n",
      " |      value is weighted by the sample weight when provided.\n",
      " |  \n",
      " |  feature_count_ : array, shape (n_classes, n_features)\n",
      " |      Number of samples encountered for each (class, feature)\n",
      " |      during fitting. This value is weighted by the sample weight when\n",
      " |      provided.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.random.randint(5, size=(6, 100))\n",
      " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      " |  >>> clf = MultinomialNB()\n",
      " |  >>> clf.fit(X, y)\n",
      " |  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      " |  >>> print(clf.predict(X[2:3]))\n",
      " |  [3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      " |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      " |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      " |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      " |  http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultinomialNB\n",
      " |      BaseDiscreteNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDiscreteNB:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Naive Bayes classifier according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance overhead hence it is better to call\n",
      " |      partial_fit on chunks of data that are as large as possible\n",
      " |      (as long as fitting in the memory budget) to hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape = [n_classes] (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseDiscreteNB:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
